{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Developing a Neural Network From Scratch\n",
    "\n",
    "## Linear and Logistic Regression\n",
    "\n",
    "### The NumPy library\n",
    "\n",
    "Develing a Neural Network ultimately boils down to linear algebra. Therefore as a result I need a data structure which models matrices. This is where the library NumPy comes in. NumPy is a library which contains a data structure called a Numpy Array. This is essentially a matrix and we will use this to model the Neural Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Hypothesis Function\n",
    "\n",
    "The hypothesis function is simply a function in which we input our X matrix and a theta matrix in order to gain the 'hypothesis'. This is what we believe the value of Y will be. \n",
    "\n",
    "#### Calculating the Hypothesis in Logistic Regression\n",
    "\n",
    "Remember that logistic regression is essentially determining the class that X belongs to. The output can only be a discrete output. Therefore for the logistic hypothesis we need to transform it to a value between 0 and 1. This is where the sigmoid function comes in.\n",
    "\n",
    "![The sigmoid function](https://miro.medium.com/max/1280/1*sOtpVYq2Msjxz51XMn1QSA.png)\n",
    "\n",
    "If the value of the sigmoid function is above 0.5 we say that X belongs to the positive class if the output is less than 0.5 we say that X belongs to the negative class.\n",
    "\n",
    "#### Calculating the Hypothesis In Linear Regression\n",
    "\n",
    "This is a much more familiar type of hypothesis. In fact most people reading this will have already implemented this without even releasing it. If you have ever drawn a line of best fit in Mathematics you have implemented lineaer regression. To calculate the hypothesis will simply multiply the matrix X by the matrix theta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hypothesis(theta, X, logistic=False):\n",
    "    \"\"\"Calculate the hypothesis for both logistic and linear regression\"\"\"\n",
    "    if logistic:\n",
    "        return 1/(1 + np.exp(-np.matmul(X, theta)))\n",
    "    else:\n",
    "        return np.matmul(X, theta)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Cost Function\n",
    "\n",
    "Now that I have identified what the hypothesis means for both linear and logistic regression I will now talk about the cost function. The cost function is a function that allows us to measure how well our model fits the data provided.\n",
    "\n",
    "For linear regression this involves measuring the difference between the hypothesis for a given X and the actual Y value for that X. In linear regression one way of computing this is by calculating the mean square distance of the hypothesis and the Y values. The cost function for Linear Regression is as follows:\n",
    "\n",
    "![The Cost Function For Linear Regression](https://lh3.googleusercontent.com/proxy/u3kKnWFubERMPyDVfE3gKAc9NTXBpmkRj49VgOS5JsL1EbnHEoIrXV96UeS0PW2-M4ei-PC8UdHltNIPE3PZNtdd8yPLJB8rxC4H0ymorCnc9C00AZJtQA)\n",
    "\n",
    "\n",
    "For logistic regression this involves something a bit more complicated. If y = 1 the cost of one data point is the negative logarithm of the hypothesis. If y = 0 is the negative logarithm of one minus the hypothesis. Putting this together means the cost function for Logistic Regression is as follows:\n",
    "\n",
    "![The Cost Funcction For Logistic Regression](https://i.stack.imgur.com/XbU4S.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function(X, Y, theta, learning_rate, regularization, logistic):\n",
    "    \"\"\"Calculates the cost and new theta for the given parameters.\n",
    "    Calculates the cost function for both logistic and linear regression.\n",
    "    The Function also performs one iteration of gradient descent.\"\"\"\n",
    "    \n",
    "    # Make a prediction using the given X and theta.\n",
    "    prediction = hypothesis(theta, X, logistic)\n",
    "\n",
    "    # Get the number of labelled data.\n",
    "    m = X.shape[0]\n",
    "\n",
    "    # Calculate the difference between the prediction matrix and the Y matrix.\n",
    "    difference = prediction - Y       \n",
    "\n",
    "    ##### Calculate the next theta using gradient descent.\n",
    "    sigma = np.dot((difference)[:,0], X).reshape((-1,1))\n",
    "\n",
    "    theta_0 = theta[0:1, :] - ((learning_rate / m) * sigma[0:1, :])\n",
    "    theta_rest = theta[1:, :] * \\\n",
    "                    (1 - (learning_rate * regularization / m)) - ((learning_rate / m) * sigma[1:,:])\n",
    "    \n",
    "    # We do not choose the first feature.\n",
    "    square_theta = theta[1:, :] * theta[1:, :]\n",
    "    square_theta = np.sum(square_theta, axis=0)\n",
    "    \n",
    "    # If we are not using a logistic model then we sum the square difference.\n",
    "    if not logistic:          \n",
    "        # Calculate the cost using the linear function.\n",
    "        square_difference = difference * difference\n",
    "        sigma = square_difference.sum()\n",
    "        cost = (1/(2*m)) * (sigma + regularization * square_theta)\n",
    "        \n",
    "    else:\n",
    "        # Calculate the cost using the logistic function.\n",
    "        log_hypoth = np.log(prediction)\n",
    "        sigma = (Y * log_hypoth + (1-Y) * np.log(1 - prediction)).sum()\n",
    "        cost = - (((1/m) * (sigma)) + (regularization/(2*m)) * square_theta)      \n",
    "\n",
    "    # Return the cost of the model and the new theta we have calculated.\n",
    "    return cost, np.insert(theta_rest, 0, theta_0, axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "\n",
    "If you read the above code you may be a little confused. I have only talked about updating theta. If so you have been paying attention to this post. The above method is a convenience method and is never called directly. The program outputs the cost and it additionally outputs a new value of theta based on the derivative of the cost. The two are interlinked so I calculate them at the same time under one function.\n",
    "\n",
    "Gradient descent in Mathematics is an algorithm to find a minima numerically. The algorithm is defined as follows:\n",
    "\n",
    "![The Gradient Descent Algorithm](https://hackernoon.com/hn-images/0*8yzvd7QZLn5T1XWg.jpg)\n",
    "\n",
    "We repeat gradient descent until theta stops updating. This in reality would take years so we declare a value in which if the difference between the previous theta value and the current theta value is less than this value then we stop converging.\n",
    "\n",
    "Alpha in the algorithm represents the learning rate. This is how far we go down the tangent of the derivative. If it is too small then gradient descent takes forever, if this is too large then gradient descent will diverge. We need to careful when implementing gradient descent as we may converge to a local minima and not a global minima.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, Y, learning_rate=0.01, max_iters=1000, regularization=0.1, logistic=False):\n",
    "    \"\"\"Returns the optimum theta using gradient descent.\n",
    "       Remember that you may need to Optimizaton    algorithms.\n",
    "       We use regularization\"\"\"\n",
    "    \n",
    "    \n",
    "    # Number of training examples.\n",
    "    m = X.shape[0]\n",
    "\n",
    "    # When true we stop calculating the value of theta.\n",
    "    done = False\n",
    "    \n",
    "    # We declare convergence if J(theta) decreases by less than below.\n",
    "    convergence = 10 ** -3\n",
    "    \n",
    "    # Assign theta to be a value of 0s initially.\n",
    "    theta = np.array([0 for x in range(X.shape[1] + 1)], dtype='float')\n",
    "    theta = np.reshape(theta, (-1,1))    \n",
    "    \n",
    "    # Need to add ones to X\n",
    "    ones = np.ones((1,m))\n",
    "    \n",
    "    # Append the columns of ones to x. This represents the bias.\n",
    "    X = np.insert(X, 0, ones, axis=1)        \n",
    "    \n",
    "    # Perform gradient descent.\n",
    "    for i in range(max_iters):\n",
    "        \n",
    "        # Output if the maximum number of iterations has been reached.\n",
    "        if i == max_iters - 1:\n",
    "            print(\"The maximum number of iterations has been reached!\")\n",
    "        \n",
    "        prev_theta = theta\n",
    "    \n",
    "        # Update the cost and theta.\n",
    "        cost, theta = cost_function(X, Y, theta, learning_rate, regularization, logistic)\n",
    "    \n",
    "        # Stop the gradient descent algorithm if we have converged.\n",
    "        if abs((theta - prev_theta).max(axis=0)) < convergence:\n",
    "            print(\"Converged!\")\n",
    "            break            \n",
    "        \n",
    "    return X, theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting and Underfitting\n",
    "\n",
    "Sometimes when we carry out gradient descent in order to obtain a value of theta which models our data the model may not be as accurate as it can be. This is due to two reasons overfitting and underfitting.\n",
    "\n",
    "Underfitting is when our computed model does not fit our data and leads to inaccuracy in the training and testing examples. When this happens we may need to add more features to gain insight into the data, change or model or implement feature scaling.\n",
    "\n",
    "Overfittting is when our computed model fits our training examples so will it does not generalise to our testing examples. To overcome this we use regularization.\n",
    "\n",
    "#### Regularization\n",
    "\n",
    "Regularization adds an additional penalty to ensure coefficients do not take exterme values. We do not perform regularization on the bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Decision Boundary\n",
    "\n",
    "I have told you what logistic regression is but not how to convert the hypothesis into a given class. We say the hypothesis belongs to the positive class if the value is greater than 0.5 and we say the hypothesis belongs to the negative class is the value is less than 0.5.\n",
    "\n",
    "The set of values in which the hypothesis computed is exactly 0.5 is called the decision boundary.\n",
    "\n",
    "![The decision boundaries](https://miro.medium.com/max/1012/1*i_oYgWjPbXbg3Z2uQLAmtw.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_class(prob):\n",
    "    \"\"\"Converts the hypothesis from a probability to a class.\"\"\"\n",
    "    return np.where(prob > 0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling\n",
    "\n",
    "Machine Learning algorithms work best when each feature is on a similar scale. If features are not in a similar scale it may result in underfitting of the data. \n",
    "\n",
    "One way of scaling features is to transform the data so that the data has a mean of 0 and a standard deviation of 1. If you have taken A-Level mathematics you will probably be familiar with this. To do this we calculate the mean of each feature and subtract each element by this mean. We then divide this by the standard deviation of that feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_scaling(X):\n",
    "    \"\"\"Uses mean normalization to scale the input.\"\"\"\n",
    "    \n",
    "    # Calculate the mean of each feature.\n",
    "    X_mean = np.mean(X, axis=0).reshape((-1,1))\n",
    "    X_std = np.std(X, axis=0).reshape((-1,1))\n",
    "\n",
    "    # If the standard deviation is 0 then we encounter problems.\n",
    "    if X_std.min(axis=0)[0] == 0:\n",
    "        return None\n",
    "    \n",
    "    return (X - X_mean.T) / X_std.T "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiclass Classification (One-vs-All)\n",
    "\n",
    "So far I have talked about only how we can determine if something belongs to the positive class and if something belongs to the negative class. I as of yet have not talked about how to predict an outcome if there is more than one class. The technique I have implemented to carry this out is One-vs-All logistic classificaiton.\n",
    "\n",
    "In this method we iterate over each class and we carry out logistic classification as normal taking that class as the positive class and every other class as the negative class. If the probability of the X input being in that class is greater than the currently highest probability of it being in another class, we update the class in which X belongs to.\n",
    "\n",
    "![one-vs-all classification](https://utkuufuk.com/2018/06/03/one-vs-all-classification/one-vs-all.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_classification_multi(X, Y, learning_rate=0.01, regularization=0.01, max_iters=1000):\n",
    "    \"\"\"Performs logistic regression multiclass classification\"\"\"\n",
    "    # We need to find the classes in Y\n",
    "    classes = np.unique(Y.flatten())\n",
    "    \n",
    "    # Iterate over the classes. Find the class with the highest probabiity.\n",
    "    # We predict that class as the class that we are going to use for predictions.\n",
    "\n",
    "    # Probabilities of each class.\n",
    "    probabilities = [[0,0] for i in range(Y.shape[0])]\n",
    "    \n",
    "    # List of the thetas we have learnt.\n",
    "    thetas = []\n",
    "\n",
    "    for i in range(classes.shape[0]):\n",
    "        # Choose close i as the positive class and the others as the negative class.\n",
    "        Y_temp = np.where(Y == i, 1, 0)\n",
    "        \n",
    "        X_temp, theta_temp = gradient_descent(X, Y_temp, learning_rate=learning_rate, \n",
    "                                              logistic=True, max_iters=max_iters, regularization=regularization)\n",
    "        probs = hypothesis(theta_temp, X_temp, True)\n",
    "        temp_classes = predict_class(probs)\n",
    "        \n",
    "        # Append theta to the list of thetas for each class.\n",
    "        theta_temp.append(theta_temp)\n",
    "        \n",
    "                \n",
    "        for j in range(len(probabilities)):\n",
    "            if probs[j, 0] > probabilities[j][0]:\n",
    "                probabilities[j] = [probs[j, 0], i]\n",
    "                \n",
    "    return thetas, probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_classification(X, Y, learning_rate=0.01, regularization=0.01, max_iters=1000):\n",
    "    \"\"\"This method is called by the user to generate theta.\"\"\"\n",
    "    X, theta = gradient_descent(X, Y, learning_rate=learning_rate, logistic=True, max_iters=max_iters, \n",
    "                               regularization=regularization)\n",
    "    probs = hypothesis(theta, X)\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Networks\n",
    "\n",
    "Now we have got to the exciting part. The part which most people came to this page wanting to know. I am talking about Neural Networks. In particular I am talking about the feedforward neural network. The bad news is that as with anything exciting it is incredibly hard to implement. But nevertheless I have a working implementation. First of all I will let you digest the cost function:\n",
    "\n",
    "![neural_networks_cost_function](https://i.stack.imgur.com/PYhrJ.png)\n",
    "\n",
    "It looks like a difficult formula but it is not too hard. In essense if we ignore paramatization we are calculating the sum of the costs for every node in the layer once we have done this we have found the cost of the model using one data sample. We then repeat this for as every data samples we have and divide by the number of samples.\n",
    "\n",
    "#### Feed Forward\n",
    "\n",
    "In the feed forward phase of the neural network we simply add a bias unit to each layer. Use a list of thetas to calculate the theta for each layer, we then apply the activation function (we're using sigmoid) for each node in that layer and so on until we get to the final layer which is our hypothesis.\n",
    "\n",
    "#### Back Propagation\n",
    "\n",
    "Back propagation is how we adjust the matrices of theta. In essense we calculate the error of the hypothesis and back propagate the error for each layer excluding the first. \n",
    "\n",
    "I know I haven't explained neural networks as detailed as I could have but it is an incredibly complicated algorithm and hard to explain abstractly. If anyone wants me to explain it in detail I would be happy to help.\n",
    "\n",
    "#### Structure of a Neural Network\n",
    "\n",
    "![structure_of_neuralnetworks](https://uc-r.github.io/public/images/analytics/deep_learning/deep_nn.png)\n",
    "\n",
    "In a neural network each layer contains a bias unit which is the first node. This is equal to 1 for all nodes. We add our data to the first layer. Between the first layer and the output layer we have the nodes called the hidden nodes. These determine the complexity of what the neural network can learn. It is important to select the number of nodes in each layer carefully to avoid overfitting and underfitting.\n",
    "\n",
    "Neural Networks can make complicated inferences about data which linear and logistic regression simply cannot.\n",
    "\n",
    "![neural_network_decision_boundary](https://i.stack.imgur.com/8oi2L.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "\n",
    "    def __init__(self, X, Y, num_features, *layers):\n",
    "        \"\"\"Sets up the hideen layers in the network and the inital values of theta.\"\"\"\n",
    "\n",
    "        self.number_of_layers = len(layers) + 1\n",
    "        self.size_of_layers = list(layers)\n",
    "        self.size_of_layers.insert(0, num_features)\n",
    "\n",
    "        self.theta = []\n",
    "        self.nodes = []\n",
    "\n",
    "        self.num_features = num_features\n",
    "\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "\n",
    "        self.regularization = 0.001\n",
    "        self.learning_rate = 0.01\n",
    "\n",
    "        self.max_iters = 1000\n",
    "\n",
    "    def sigmoid_activation(self, np_array):\n",
    "        \"\"\"Apply sigmoid activation on the given layer.\"\"\"\n",
    "\n",
    "        # Need to prevent overflow error.\n",
    "        signal = np.clip(np_array, -500, 500)\n",
    "\n",
    "        return 1 / (1 + np.exp(-signal))\n",
    "\n",
    "    def forward_propagation(self, X_inp):\n",
    "        \"\"\"Calcules the hypothesis via feed forward propagation.\"\"\"\n",
    "\n",
    "        # Insert the X_input into X\n",
    "        self.nodes[0][1:, :] = X_inp\n",
    "\n",
    "        for i in range(len(self.theta)):\n",
    "            if i < len(self.theta) - 1:\n",
    "                self.nodes[i + 1][1:, :] = np.matmul(self.theta[i], self.nodes[i])\n",
    "                self.nodes[i + 1][1:, :] = self.sigmoid_activation(self.nodes[i + 1][1:, :])\n",
    "            else:\n",
    "                self.nodes[i + 1] = np.matmul(self.theta[i], self.nodes[i])\n",
    "                self.nodes[i + 1] = self.sigmoid_activation(self.nodes[i + 1])\n",
    "\n",
    "        return self.nodes[-1]\n",
    "\n",
    "    def backward_propagation(self, hypothesis, Y_test, accumulator):\n",
    "        \"\"\"Perform backwards propagation which is used to trin the neural network.\"\"\"\n",
    "\n",
    "        # Going to do backwards propagation again.\n",
    "        last_error = hypothesis - Y_test.reshape((-1, 1))\n",
    "\n",
    "        # Create a list of the errors.\n",
    "        errors = [np.zeros((self.nodes[i].shape)) for i in range(1, len(self.nodes) - 1)]\n",
    "        errors.append(last_error)\n",
    "\n",
    "        # Now we need to actually perform the backwards propagation.\n",
    "        for i in reversed(range(1, len(self.nodes) - 1)):\n",
    "            # No bias unit on the final layer.\n",
    "            if i == len(self.nodes) - 2:\n",
    "                temp_error = np.matmul(self.theta[i].T, errors[i])\n",
    "            else:\n",
    "                temp_error = np.matmul(self.theta[i].T, errors[i][1:, :])\n",
    "\n",
    "            errors[i - 1] = temp_error * (self.nodes[i] * (1 - self.nodes[i]))\n",
    "\n",
    "        # Now we have a list of all the errors.\n",
    "        for i in range(len(accumulator)):\n",
    "            if i == len(accumulator) - 1:\n",
    "                accumulator[i] = accumulator[i] + np.matmul(errors[i], self.nodes[i].T)\n",
    "            else:\n",
    "                accumulator[i] = accumulator[i] + np.matmul(errors[i][1:, :], self.nodes[i].T)\n",
    "\n",
    "        return accumulator\n",
    "\n",
    "    def gradient_descent(self, delt):\n",
    "        \"\"\"This method performs gradient descent using the given delt.\"\"\"\n",
    "\n",
    "        max_descent = 0\n",
    "        prev_theta = self.theta\n",
    "\n",
    "        for i in range(len(delt)):\n",
    "            self.theta[i] = self.theta[i] - self.learning_rate * delt[i]\n",
    "            difference = prev_theta[i] - self.theta[i]\n",
    "\n",
    "            # Get the maximum\n",
    "            if np.max(difference) > max_descent:\n",
    "                max_descent = np.max(difference)\n",
    "\n",
    "        # if max_descent < 10 ** -4:\n",
    "        #   print(\"Converged!\")\n",
    "        #  return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = self.forward_propagation(X)\n",
    "\n",
    "        # Get the largest prediction\n",
    "\n",
    "        return predictions\n",
    "\n",
    "    def train_neural_network(self, X, Y):\n",
    "        \"\"\"Trains the neural network model.\n",
    "        Layers passed in contains two keys,\n",
    "        number of layers and array containing\n",
    "        the size of each layer.\"\"\"\n",
    "\n",
    "        # Now we need to instantiate the layers.\n",
    "        # We need to use random initialization this time.\n",
    "        # Zero initialization will not work.\n",
    "\n",
    "        # We're going to find epsilon for random initialisation\n",
    "        epsilon = 0.5\n",
    "\n",
    "        self.theta = []\n",
    "        self.nodes = []  # We're also creating the hidden node matrices.\n",
    "\n",
    "        # Append the X input nodes into the activation nodes.\n",
    "        self.nodes.append(np.array([0 for i in range(self.num_features)], dtype='float').reshape((-1, 1)))\n",
    "        # We are using a list and not a 2 dimensional matrix as a two dimensional\n",
    "        # so that we can store different number of nodes in different layers.\n",
    "        self.nodes[0] = np.insert(self.nodes[0], 0, 1).reshape((-1, 1))\n",
    "\n",
    "        for i in range(self.number_of_layers - 1):\n",
    "            self.theta.append(np.random.rand(self.size_of_layers[i + 1],\n",
    "                                             self.size_of_layers[i] + 1) * (2 * epsilon) - (epsilon))\n",
    "            self.nodes.append(np.zeros(self.size_of_layers[i + 1]))\n",
    "\n",
    "            if i != self.number_of_layers - 2:\n",
    "                self.nodes[i + 1] = np.insert(self.nodes[i + 1], 0, 1).reshape((-1, 1))\n",
    "                # Do not want to insert a 1 in the last layer\n",
    "            else:\n",
    "                self.nodes[i + 1] = self.nodes[i + 1].reshape((-1, 1))\n",
    "\n",
    "        # We need an accumulator.\n",
    "        accumulator = [np.zeros((self.theta[i].shape[0], self.theta[i].shape[1])) \\\n",
    "                       for i in range(len(self.theta))]\n",
    "\n",
    "        # Now we need to perform forward propagation.\n",
    "        # Perform backward propagatio on all the examples.\n",
    "        for j in range(self.max_iters):\n",
    "            for i in range(X.shape[0]):\n",
    "                hypothesis = self.forward_propagation(X[i, :].reshape((-1, 1)))\n",
    "                accumulator = self.backward_propagation(hypothesis, Y[i, :], accumulator)\n",
    "\n",
    "            m = X.shape[0]\n",
    "\n",
    "            delt = [(1 / m) * accumulator[i] for i in range(len(self.theta))]\n",
    "\n",
    "            # Add regularization. We do not regularize the bias term.\n",
    "            for i in range(len(delt)):\n",
    "                delt[i][:, 1:] = delt[i][:, 1:] + (self.regularization / m) * self.theta[i][:, 1:]\n",
    "\n",
    "            done = self.gradient_descent(delt)\n",
    "\n",
    "            if j == self.max_iters - 1:\n",
    "                print(\"The maximum number of iterations has been reached!\")\n",
    "\n",
    "            if done:\n",
    "                print(\"Converged!\")\n",
    "                break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "theta0.csv not found.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-368-54cb2963ca0e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;31m# nn.save_theta()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_theta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"theta0.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"theta1.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Machine_Learning/PythonMachineLearning/Introduction_To_Machine_Learning/NeuralNet/NeuralNet.py\u001b[0m in \u001b[0;36mload_theta\u001b[0;34m(self, *files)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m \"\"\"\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ml/lib/python3.8/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mloadtxt\u001b[0;34m(fname, dtype, comments, delimiter, converters, skiprows, usecols, unpack, ndmin, encoding, max_rows)\u001b[0m\n\u001b[1;32m    979\u001b[0m             \u001b[0mfname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_string_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 981\u001b[0;31m             \u001b[0mfh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_datasource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    982\u001b[0m             \u001b[0mfencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'encoding'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'latin1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    983\u001b[0m             \u001b[0mfh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ml/lib/python3.8/site-packages/numpy/lib/_datasource.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(path, mode, destpath, encoding, newline)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataSource\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdestpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnewline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ml/lib/python3.8/site-packages/numpy/lib/_datasource.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, path, mode, encoding, newline)\u001b[0m\n\u001b[1;32m    621\u001b[0m                                       encoding=encoding, newline=newline)\n\u001b[1;32m    622\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 623\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s not found.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    624\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: theta0.csv not found."
     ]
    }
   ],
   "source": [
    "from mnist import MNIST\n",
    "import numpy as np\n",
    "import random\n",
    "import NeuralNet\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mndata = MNIST('samples')\n",
    "\n",
    "images, labels = mndata.load_training()\n",
    "\n",
    "mntesting = MNIST('testing')\n",
    "\n",
    "image_test, image_label = mntesting.load_testing()\n",
    "\n",
    "# nn = NeuralNet(images, labels, images.shape[1], 100, 100, labels.shape[1])\n",
    "\n",
    "# nn.train_neural_network(images, labels)\n",
    "\n",
    "np_testing_X = np.array([images[i] for i in range(len(images))])\n",
    "np_testing_Y = np.zeros((len(image_label), 10))\n",
    "\n",
    "for i in range(np_testing_Y.shape[0]):\n",
    "    index = image_label[i]\n",
    "    np_testing_Y[i, index] = 1\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(np_testing_X[:5000, :], np_testing_Y[:5000, :], random_state=69)\n",
    "\n",
    "nn = NeuralNet.NeuralNetwork(X_train, y_train, np_testing_X.shape[1], 800, np_testing_Y.shape[1])\n",
    "\n",
    "# nn.train_neural_network(X_test, y_test)\n",
    "\n",
    "# Just gonna save the theta in a file.\n",
    "# nn.save_theta()\n",
    "\n",
    "nn.load_theta(\"theta0.csv\", \"theta1.csv\")\n",
    "\n",
    "\n",
    "# Pick a random index.\n",
    "\n",
    "index = random.randrange(0, len(X_train))\n",
    "\n",
    "plt.imshow(X_train[index].reshape((28,28)), cmap='gray')\n",
    "\n",
    "prediction = nn.predict(X_train[index].reshape((-1,1)))\n",
    "prediction = np.argmax(prediction)\n",
    "\n",
    "\n",
    "plt.xlabel(\"Prediction: \" + str(prediction))\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The End\n",
    "\n",
    "Thank you for putting up with me in this post. Have a good one.\n",
    "\n",
    "![san_miguel](miguel.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
